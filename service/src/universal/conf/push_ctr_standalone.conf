akka {
  loglevel = INFO
  loggers = ["akka.event.slf4j.Slf4jLogger"]
  event-handlers = ["akka.event.slf4j.Slf4jEventHandler"]

  log-config-on-start = on
}

akka.cluster {
  auto-down-unreachable-after = off
  metrics.enabled = off
  failure-detector {
    heartbeat-interval = 100 s  # default 1s
    acceptable-heartbeat-pause = 250 s # default 3 s
    threshold = 8.0    # default 8.0
  }

  scheduler {
    tick-duration = 9ms # default 33ms
    ticks-per-wheel = 512 # default 512
  }
  roles = ["entity"]
  use-dispatcher = cluster-dispatcher
  contrib.cluster.sharding.role = "entity"
  contrib.cluster.pub-sub.gossip-interval = 3s  # default 1s
}

cluster-dispatcher {
  type = "Dispatcher"
  executor = "fork-join-executor"
  fork-join-executor {
    parallelism-min = 2
    parallelism-max = 4
  }
}

akka.scheduler {
  tick-duration = 10ms # default 10ms
  ticks-per-wheel = 512 # default 512
}

akka.io {
  tcp {
    nr-of-selectors = 2
    max-channels = unlimited
  }
}

akka.remote {
  #log-remote-lifecycle-events = on
  transport-failure-detector {
    # since TCP is used, increase heartbeat-interval generously
    # how often the node publishes heartbeats
    heartbeat-interval = 100 s   # default 4s
    # if after this duration no heartbeat has been received, you have a problem
    acceptable-heartbeat-pause = 250 s  # default 10s
  }
  netty {
    server-socket-worker-pool {
      pool-size-min = 2 # default 2
      pool-size-factor = 1.0 # default 1.0
      pool-size-max = 24 # default 2
    }
  }
  retry-gate-closed-for = 2 s  # default 5s
}

akka.contrib.cluster.pub-sub {
  gossip-interval = 3s  # default 1s
}

akka.extensions = ["akka.contrib.pattern.ClusterReceptionistExtension"]

# check the reference.conf in spray-can/src/main/resources for all defined settings
spray.can.server {
  # uncomment the next line for making this an HTTPS example
  #ssl-encryption = on
  idle-timeout = infinite
  registration-timeout = 100s
  request-timeout = infinite

  request-chunk-aggregation-limit = 0

  parsing.max-content-length = 5g
  parsing.incoming-auto-chunking-threshold-size = 45k
}

spray.can.client {
  idle-timeout = infinite
  connecting-timeout = 100s
}

akka.actor.provider = "akka.cluster.ClusterActorRefProvider"

#### environment related

akka.remote.netty.tcp.hostname = ${?cluster_hostname}
akka.remote.netty.tcp.port = 2551
akka.cluster.roles = ["entity"]

web.hostname = ${?cluster_hostname}
web.port = 8080

akka.persistence.journal.max-message-batch-size = 400

test {
  web.port = 8083

  akka.cluster.seed-nodes = [
    "akka.tcp://PushCtrService@127.0.0.1:2551"
  ]

  akka.persistence.journal.leveldb.dir = "target/journal"
  akka.persistence.snapshot-store.local.dir = "target/snapshots"
}

dev {
  web.port = 8083

  akka.cluster.seed-nodes = [
    "akka.tcp://PushCtrService@127.0.0.1:2551"
  ]

  akka.persistence.journal.plugin = "hbase-journal"
  akka.persistence.snapshot-store.plugin = "hadoop-snapshot-store"
  hbase-journal {

    # Partitions will be used to avoid the "hot write region" problem.
    # Set this to a number greater than the expected number of regions of your table.
    # WARNING: It is not supported to change the partition count when already written to a table (could miss some records)
    partition.count = 10

    # All these settings will be set on the underlying Hadoop Configuration
    hadoop-pass-through {
      hbase.zookeeper.quorum = "127.0.0.1:2181"
      zookeeper.znode.parent = "/hbase"
    }

    # Name of the table to be used by the journal
    table = "sc:pushctr_journal"

    # Name of the family to be used by the journal
    family = "journal"

    # When performing scans, how many items to we want to obtain per one next(N) call.
    # This most notably affects the speed at which message replay progresses, fine tune this to match your cluster.
    scan-batch-size = 200

    # Dispatcher for fetching and replaying messages
    replay-dispatcher = "akka-hbase-persistence-replay-dispatcher"

    # Default dispatcher used by plugin
    plugin-dispatcher = "akka-hbase-persistence-dispatcher"
  }

  hadoop-snapshot-store {
    # select your preferred implementation based on your needs
    #
    # * HBase - snapshots stored together with snapshots; Snapshot size limited by Int.MaxValue bytes (currently)
    #
    # * HDFS *deprecated, will be separate project* - can handle HUGE snapshots;
    #          Can be easily dumped to local filesystem using Hadoop CL tools
    #          impl class is "akka.persistence.hbase.snapshot.HdfsSnapshotter"
    mode = "hbase"

    hbase {
      # Name of the table to be used by the journal
      table = "sc:pushctr_snapshot"

      # Name of the family to be used by the journal
      family = "snapshot"
    }

    # Default dispatcher used by plugin
    plugin-dispatcher = "akka-hbase-persistence-dispatcher"
  }

  akka-hbase-persistence-dispatcher {
    type = Dispatcher
    executor = "thread-pool-executor"
    thread-pool-executor {
      core-pool-size-min = 2
      core-pool-size-factor = 24.0
      core-pool-size-max = 5
    }
    throughput = 200
  }


  akka-hbase-persistence-replay-dispatcher {
    type = Dispatcher
    executor = "thread-pool-executor"
    thread-pool-executor {
      core-pool-size-min = 2
      core-pool-size-factor = 24.0
      core-pool-size-max = 5
    }
    throughput = 200
  }
}

stage {
  akka.cluster.seed-nodes = [
    "akka.tcp://PushCtrService@127.0.0.1:2551"
  ]
  akka.persistence.journal.plugin = "cassandra-journal"
  akka.persistence.snapshot-store.plugin = "cassandra-snapshot-store"
  cassandra-journal {
    contact-points = ["127.0.0.1"]
    keyspace = "statusPushCtr"
  }
  cassandra-snapshot-store {
    contact-points = ["127.0.0.1"]
    keyspace = "statusPushCtr"
  }
}

prod {
  akka.cluster.seed-nodes = [
    "akka.tcp://PushCtrService@127.0.0.1:2551"
  ]

  akka.persistence.journal.plugin = "hbase-journal"
  akka.persistence.snapshot-store.plugin = "hadoop-snapshot-store"

  hbase-journal {

    # Partitions will be used to avoid the "hot write region" problem.
    # Set this to a number greater than the expected number of regions of your table.
    # WARNING: It is not supported to change the partition count when already written to a table (could miss some records)
    partition.count = 10

    # All these settings will be set on the underlying Hadoop Configuration
    hadoop-pass-through {
      hbase.zookeeper.quorum = "127.0.0.1:2181"
      zookeeper.znode.parent = "/online-hbase"
    }

    # Name of the table to be used by the journal
    table = "sc:pushctr_journal"

    # Name of the family to be used by the journal
    family = "journal"

    # When performing scans, how many items to we want to obtain per one next(N) call.
    # This most notably affects the speed at which message replay progresses, fine tune this to match your cluster.
    scan-batch-size = 400

    # Dispatcher for fetching and replaying messages
    replay-dispatcher = "akka-hbase-persistence-replay-dispatcher"

    # Default dispatcher used by plugin
    plugin-dispatcher = "akka-hbase-persistence-dispatcher"
  }

  hadoop-snapshot-store {
    # select your preferred implementation based on your needs
    #
    # * HBase - snapshots stored together with snapshots; Snapshot size limited by Int.MaxValue bytes (currently)
    #
    # * HDFS *deprecated, will be separate project* - can handle HUGE snapshots;
    #          Can be easily dumped to local filesystem using Hadoop CL tools
    #          impl class is "akka.persistence.hbase.snapshot.HdfsSnapshotter"
    mode = "hbase"

    hbase {
      # Name of the table to be used by the journal
      table = "sc:pushctr_snapshot"

      # Name of the family to be used by the journal
      family = "snapshot"
    }

    # Default dispatcher used by plugin
    plugin-dispatcher = "akka-hbase-persistence-dispatcher"
  }

  akka-hbase-persistence-dispatcher {
    type = Dispatcher
    executor = "thread-pool-executor"
    thread-pool-executor {
      core-pool-size-min = 16
      core-pool-size-factor = 256.0
      core-pool-size-max = 32
    }
    throughput = 1000
  }

  akka-hbase-persistence-replay-dispatcher {
    type = Dispatcher
    executor = "thread-pool-executor"
    thread-pool-executor {
      core-pool-size-min = 16
      core-pool-size-factor = 256.0
      core-pool-size-max = 32
    }
    throughput = 1000
  }
}
